{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report_material/feup_logo_old.png\" width=\"30%\" align=\"center\"/>\n",
    "\n",
    "<h1 style=\"text-align:center\">Neural Networks for the Prediction of Anuran Species (E3.)</h1>\n",
    "<h3 style=\"text-align:center\">Final Delivery</h3>\n",
    "<br/><br/>\n",
    "<p style=\"text-align:center\">Artificial Intelligence (IART)</p>\n",
    "<p style=\"text-align:center\">3<sup>rd</sup> year in Master in Informatics and Computing Engineering</p>\n",
    "\n",
    "### Authors\n",
    "\n",
    "* Miguel Ramalho    - 201403027 - [m.ramalho@fe.up.pt](mailto:up201403027@fe.up.pt)\n",
    "* Rostyslav Khoptiy - 201506219 - [up201506219@fe.up.pt](mailto:up201506219@fe.up.pt)\n",
    "\n",
    "<!--spellchecking: https://stackoverflow.com/a/48710561/6196010 -->\n",
    "\n",
    "<div style=\"text-align:right\"><i>April 8<sup>th</sup>, 2018</i></div>\n",
    "<br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IART\n",
    "\n",
    "* [GitHub repository](https://github.com/msramalho/iart)\n",
    "* [UCI source page](http://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29)\n",
    "* [README](dataset/README.ipynb) of the dataset\n",
    "* [Dataset file](dataset/frogs.csv)\n",
    "* [Automatic Classification of Anuran Sounds Using Convolutional Neural Networks](report_material/papers/Automatic%20Classification%20of%20Anuran%20Sounds%20Using.pdf)\n",
    "* [Tensorflow tutorial](https://www.tensorflow.org/get_started/premade_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import io # to pass data as StringIO object\n",
    "import time # to set the random seed for train and test sampling\n",
    "import matplotlib.pyplot as plt # to display plots\n",
    "from sklearn.model_selection import KFold, StratifiedKFold # for cross-validation, needs scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/frogs.csv\" # the path to the dataset file\n",
    "N_FEATURES   = 22 # the number of features in the dataset\n",
    "LABEL_INDEX  = 24 # the index in the dataset where the label to predict is\n",
    "BATCH_SIZE   = 100 # batch size for the train, evaluation and prediction\n",
    "NUM_CLASSES  = 10 # last layer, num classifications\n",
    "NUM_ITEMS    = 7195 #Num items in dataset\n",
    "CROSS_VALIDATION_FUNCTION = StratifiedKFold # use KFold to ignore class representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "In this section we include functions that are directed at helping the visualisation of the project, the manipulation of data and the creation, training and evaluation of models. Here functions that start with an underscore (_) are purely 'private' helper functions and shouldn't be used or even considered for the evaluation of the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart Drawing and Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# receives paddings and a list @layer_sizes with the number of nodes to draw\n",
    "def _draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    # https://gist.github.com/craffel/2d727968c3aaebd10359\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(max(len(layer_sizes) - 1, 1))\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size):\n",
    "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "                                color='w', ec='k', zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "    # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                  [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                ax.add_artist(line)\n",
    "# helper function to draw a neural network\n",
    "def drawNN(hidden_layers, figsize=(12, 12)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.gca()\n",
    "    ax.axis('off')\n",
    "    _draw_neural_net(ax, .1, .9, .1, .9, hidden_layers)\n",
    "#     fig.savefig('nn.png') # uncomment to save the figure to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an abstract histogram\n",
    "def _draw_label_distribution(label_column, title_text):\n",
    "    label_count = pd.Series.value_counts(label_column, normalize=True, sort=False)\n",
    "    label_count = label_count.sort_index()\n",
    "    label_count.plot(kind='barh', title=title_text, stacked=True, fontsize=15, figsize=(8,8), color=\"darkblue\" )\n",
    "\n",
    "# create a histogram with the distribution of species\n",
    "def draw_species_distribution_chart(df, title_text=\"Species Dataset Distribution\"):\n",
    "    species_column = df['Species']\n",
    "    _draw_label_distribution(species_column, title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Deep Neural Network Models \n",
    " * `my_feature_columns`        - the features that the model should receive as input\n",
    " * `inner_layers_architecture` - a list of N hidden layers where N[i] is the number of nodes of the layer i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN(my_feature_columns, inner_layers_architecture, save_folder_path=None, num_classes=NUM_CLASSES):\n",
    "    # Build a DNN (Deep Neural Network) with 1 hidden layer with 10 nodes\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "            feature_columns = my_feature_columns,\n",
    "            hidden_units = inner_layers_architecture, # 1:-1 removes the first and last elements of the architecture\n",
    "            n_classes = num_classes, # num nodes of final layer\n",
    "            model_dir = save_folder_path) # value None means it will be saved in temp, \n",
    "                                            # no need to generate path for each iteration\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_PARAM = NUM_ITEMS * 10 #df.shape[0] * 10\n",
    "# helper function to yield a part of the training data to be used by TF's classifier.train method\n",
    "def _train_input_fn(features, labels):\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    \n",
    "    # Shuffle, repeat, and batch the examples. The shuffle argument should be larger than the number of examples\n",
    "    return dataset.shuffle(SHUFFLE_PARAM).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model, given the train data and the number of steps\n",
    "def train_model(classifier, train_x, numeric_train_y, num_steps=1000):\n",
    "    classifier.train(input_fn=lambda:_train_input_fn(train_x, numeric_train_y), steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for the classifier.evaluate method\n",
    "def _eval_input_fn(features, labels=None):\n",
    "    inputs = features if labels is None else (features, labels)\n",
    "    \n",
    "    # Convert inputs to a tf.dataset object. Batch the examples.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs).batch(BATCH_SIZE)\n",
    "\n",
    "    # Return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "# returns a tuple (eval_results, global_step)\n",
    "def evaluate_model(classifier, test_x, numeric_test_y):\n",
    "    return classifier.evaluate(input_fn=lambda:_eval_input_fn(dict(test_x), numeric_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a list of strings into a list of IDs (numerical values) used for the classifier\n",
    "def _convert_labels_to_number(labels):\n",
    "    unique_labels = list(set(labels)) # list of unique values\n",
    "    return [unique_labels.index(l) for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_TF_data(train, test):\n",
    "    # split dataframes into X (features) and y (classes) - use species as label\n",
    "    train_x, train_y = train.iloc[:,:N_FEATURES], train.iloc[:,LABEL_INDEX:LABEL_INDEX+1]\n",
    "    test_x, test_y = test.iloc[:,:N_FEATURES], test.iloc[:,LABEL_INDEX:LABEL_INDEX+1]\n",
    "    \n",
    "    # for the 10 species, get a numerical mapping of the text name of that species (needed for DNN)\n",
    "    numeric_train_y = _convert_labels_to_number(train_y.iloc[:,0])\n",
    "    numeric_test_y  = _convert_labels_to_number(test_y.iloc[:,0])\n",
    "    \n",
    "    return (train_x, numeric_train_y, test_x, numeric_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features MFCCs_1 to MFCCs_22 into Tensorflow feature columns\n",
    "def get_TF_feature_columns(df):\n",
    "    data_df = df.iloc[:,:N_FEATURES]\n",
    "    return [tf.feature_column.numeric_column(key=key) for key in list(data_df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple data splitting into Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a single iteration\n",
    "def simple_data_split_generator(df, labels, train_fraction):\n",
    "    train=df.sample(frac=train_fraction, random_state=int(time.time()))\n",
    "    test=df.drop(train.index)\n",
    "    yield _format_TF_data(train, test) # in a for loop gives one single loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation splitting into Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data_by_indices(train_indices, test_indices):\n",
    "    train = df.iloc[train_indices]\n",
    "    test = df.iloc[test_indices]\n",
    "    return _format_TF_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_k_fold_CV_split_indices(df, labels, num_folds = 3):\n",
    "    k_fold = CROSS_VALIDATION_FUNCTION(n_splits = num_folds, shuffle=True)\n",
    "    return k_fold.split(df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldCV_tf_data_generator(df, labels, train_fraction):\n",
    "    num_folds = int(1.0 / (1.0 - train_fraction))\n",
    "    for train_indices, test_indices in _get_k_fold_CV_split_indices(df, labels, num_folds):\n",
    "        yield(_split_data_by_indices(train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning\n",
    "Given a dataset, the respective labels, a function to split the data and another to calculate the number of steps, this function iterates over all the defined architectures in **INNER_LAYERS** and over all the **NUM_STEPS** and over all the  **TRAIN_FRACTIONS** and does the following:\n",
    " * Split the data accordingly\n",
    " * Train the model\n",
    " * Evaluate the model\n",
    " * Report the results\n",
    " \n",
    "This can be used for quick comparison of multiple combinations of architectures, number of steps and train fractions, essentially tuning **hyper-parameters**. More can hyper-parameters can be added if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_iterations(df, labels, data_generator, num_steps_calculator):\n",
    "    evaluations = []\n",
    "    for inner_architecture in INNER_LAYERS:\n",
    "        #drawNN([N_FEATURES] + inner_architecture + [NUM_CLASSES])\n",
    "        for num_steps in NUM_STEPS:\n",
    "            for train_fraction in TRAIN_FRACTIONS:\n",
    "                my_feature_columns = get_TF_feature_columns(df) # seems to need to generate one for each estimator\n",
    "                classifier = create_DNN(my_feature_columns, inner_architecture)\n",
    "                num_steps = num_steps_calculator(num_steps, train_fraction)\n",
    "                for (train_x, numeric_train_y, test_x, numeric_test_y) in data_generator(df, labels, train_fraction):\n",
    "                    train_model(classifier, train_x, numeric_train_y, num_steps)\n",
    "                    eval_result = evaluate_model(classifier, test_x, numeric_test_y)\n",
    "                    # evaluation results\n",
    "                    # can change here what the function returns, k-fold might need to \"unite\" the different folds-iterations\n",
    "                    # or simply save the last one\n",
    "                    # maybe add a function to further process the classifier results\n",
    "                    result_tup = ( (inner_architecture, train_fraction, num_steps), eval_result )\n",
    "                    evaluations.append(result_tup)\n",
    "    return evaluations # evaluations can have duplicate 'keys' if k-fold is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely temporary\n",
    "def display_model_results(evaluations):\n",
    "    # show results, statistics\n",
    "    print('\\n\\n\\tNum Results: %d' % len(evaluations) )\n",
    "    print('>> (inner_arch, train_frac, num_steps) , Accuracy' )\n",
    "    for ( iteration_tup, eval_result ) in evaluations:\n",
    "        print(iteration_tup, ',', eval_result['accuracy'])\n",
    "    #     print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from csv to pandas Dataframe\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df.columns = df.columns.str.replace(\" \", \"\") # remove whitespace, needed for TF\n",
    "\n",
    "print(\"Got %d examples from the .csv\" % df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a mapping of all the labels in the Dataset into its integer values (required for Stratified Cross Validation)\n",
    "labels = _convert_labels_to_number(df.iloc[:,LABEL_INDEX:LABEL_INDEX+1].iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw dataset distribution\n",
    "draw_species_distribution_chart(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterations\n",
    "\n",
    "# modify here to set model configurations\n",
    "INNER_LAYERS = [\n",
    "    [10],\n",
    "#     [100],\n",
    "#     [20, 20],\n",
    "#     [100, 100],\n",
    "#     [10, 10, 10],\n",
    "]\n",
    "\n",
    "TRAIN_FRACTIONS = [\n",
    "    0.8,\n",
    "#     0.5,\n",
    "]\n",
    "\n",
    "NUM_STEPS = [\n",
    "    100, \n",
    "    200,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial approach to dataset splitting\n",
    "Random split into train and test\n",
    "\n",
    "**Warning:** Depending on the complexity of the architecture and the number of steps in the training, this operation may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this may take a while\n",
    "steps_calc = lambda num_steps, train_fraction: num_steps\n",
    "evaluations = run_model_iterations(df, labels, simple_data_split_generator, steps_calc)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple data splitting results\n",
    "display_model_results(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation approach to dataset splitting\n",
    "In **k-fold** cross-validation, the original sample is randomly partitioned into k equal size subsamples.\n",
    "\n",
    "In **stratified k-fold** cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds.\n",
    "\n",
    "To use one or the other, update the global setting `CROSS_VALIDATION_FUNCTION` in the [Initialize constants](#Initialize-constants) section\n",
    "\n",
    "**Warning:** Depending on the complexity of the architecture and the number of steps in the training, this operation may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "# this may take a while\n",
    "\n",
    "steps_calc = lambda num_steps, train_fraction: int(num_steps * (1.0 - train_fraction))\n",
    "# adjustment for k-fold: num_total_model_steps = num_steps * num_folds\n",
    "kfcv_evaluations = run_model_iterations(df, labels, kFoldCV_tf_data_generator, steps_calc)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold Cross-validation data splitting results\n",
    "display_model_results(kfcv_evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletes temporary files created by TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows deleting models save folder in windows\n",
    "tf.summary.FileWriterCache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "temp_dir = Path(tempfile.gettempdir())\n",
    "dir_list = [f for f in temp_dir.glob('tmp*') if f.is_dir()] \n",
    "for dir in dir_list:\n",
    "    shutil.rmtree(dir, ignore_errors=True)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
